{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 何謂反向傳播\n",
    "> 反向傳播（BP：Backpropagation）是「誤差反向傳播」的簡稱，是⼀種與最優化⽅法（如梯度下降法）結合使⽤的，該⽅法對網路中所有權重計算損失函數的梯度。這個梯度會反饋給最優化⽅法，⽤來更新權值以最⼩化損失函數。\n",
    "\n",
    "> 反向傳播要求有對每個輸入值想得到的已知輸出，來計算損失函數梯度。因此，它通常被認為是⼀種監督式學習⽅法，可以對每層疊代計算梯度。反向傳播要求⼈⼯神經元（或「節點」）的啟動函數可微。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以預測水果銷售為例\n",
    "> ⽔果銷售所應給付的價格決定因⼦\n",
    "- 數量(顆數或是單位重量)\n",
    "- 單價\n",
    "- 稅⾦\n",
    "\n",
    "> 建立運算單元：\n",
    "- 稅⾦是恆定的， 可以當成是 Bias， 給定 TAX\n",
    "- Input-1：數量，給定 X\n",
    "- Input-2：單價， 給定 Y\n",
    "- Output：f(x) = X * Y * TAX\n",
    "\n",
    "<img src=\"imgs/keras08_backpropagation_fruit.png\" width=\"80%\">\n",
    "<img src=\"imgs/keras08_backpropagation_fruit1.png\" width=\"80%\">\n",
    "<img src=\"imgs/keras08_backpropagation_fruit2.png\" width=\"80%\">\n",
    "<img src=\"imgs/keras08_backpropagation_fruit3.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP神經網絡\n",
    "#### BP 神經網路是⼀種按照逆向傳播算法訓練的多層前饋神經網路\n",
    "> 優點：\n",
    "- 具有任意複雜的模式分類能⼒和優良的多維函數映射能⼒，解決了簡單感知器不能解決的異或或者⼀些其他的問題。\n",
    "- 從結構上講，BP 神經網路具有輸入層、隱含層和輸出層。\n",
    "- 從本質上講，BP 算法就是以網路誤差平⽅⽬標函數、採⽤梯度下降法來計算⽬標函數的最⼩值。\n",
    "\n",
    "> 缺點：\n",
    "- 學習速度慢，即使是⼀個簡單的過程，也需要幾百次甚⾄上千次的學習才能收斂。\n",
    "- 容易陷入局部極⼩值。\n",
    "- 網路層數、神經元個數的選擇沒有相應的理論指導。\n",
    "- 網路推廣能⼒有限。\n",
    "\n",
    "> 應用：\n",
    "- 函數逼近\n",
    "- 模式識別\n",
    "- 分類\n",
    "- 數據壓縮\n",
    "\n",
    "## 學習過程\n",
    "#### 第一階段：解函數微分\n",
    "> 每次疊代中的傳播環節包含兩步：\n",
    "- （前向傳播階段）將訓練輸入送入網路以獲得啟動響應；\n",
    "- （反向傳播階段）將啟動響應同訓練輸入對應的⽬標輸出求差，從⽽獲得輸出層和隱藏層的響應誤差。\n",
    "#### 第2階段：權重更新\n",
    "> Follow Gradient Descent\n",
    "- 第 1 和第 2 階段可以反覆循環疊代，直到網路對輸入的響應達到滿意的預定的⽬標範圍為⽌。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課程目標:\n",
    "    \n",
    "    了解反向傳播的原理與在神經網路推倒過程中的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例重點:\n",
    "\n",
    "    (1)自定義神經網路架構\n",
    "    \n",
    "    (2) 初始值設定\n",
    "    \n",
    "    (3) 使用範例\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義並建立一神經網路\n",
    "class mul_layer():\n",
    "    def _ini_(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始值設定\n",
    "n_X = 2\n",
    "price_Y = 100\n",
    "b_TAX = 1.1\n",
    "\n",
    "# 指定Build _Network組合\n",
    "mul_fruit_layer = mul_layer()\n",
    "mul_tax_layer = mul_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以購買水果為例:\n",
    "    \n",
    "    付費總價格是根據水果價格, 稅金變動而受影響\n",
    "    \n",
    "    水果價格是根據購買數量與單品價格而變動\n",
    "    \n",
    "    可以利用每一個cell (cell - 1: 水果價格; cell - 2: 付費總價格), 推導微分的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward \n",
    "fruit_price = mul_fruit_layer.forward(price_Y, n_X)\n",
    "total_price = mul_tax_layer.forward(fruit_price, b_TAX)\n",
    "\n",
    "#backward \n",
    "dtotal_price = 1 #this is linear function, which y=x, dy/dx=1\n",
    "d_fruit_price, d_b_TAX = mul_tax_layer.backward(dtotal_price)\n",
    "d_price_Y, d_n_X =  mul_tax_layer.backward(d_fruit_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruit price: 200\n",
      "針對所有水果價格微分, 得到 TAX: 1.100000\n"
     ]
    }
   ],
   "source": [
    "#result\n",
    "print(\"fruit price: %i\"%fruit_price)\n",
    "print(\"針對所有水果價格微分, 得到 TAX: %2f\" %d_fruit_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP 演算法訓練的神經網路\n",
    "\n",
    "\n",
    "目標: 嘗試著用輸入去預測輸出, 了解權重的更新\n",
    " \n",
    "考慮以上情形：\n",
    "給定三row輸入，試著去預測對應的一row輸出。\n",
    "\n",
    "我們可以通過簡單測量輸入與輸出值的資料來解決這一問題。\n",
    "\n",
    "最左邊的一 row 輸入值和輸出值是完美匹配/完全相關的。\n",
    "\n",
    "反向傳播演算法便是通過這種方式來衡量資料間統計關係進而得到模型的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 參數定義\n",
    "\n",
    "X\t輸入資料集，形式為矩陣，每 1 column 代表 1 個訓練樣本。\n",
    "\n",
    "y\t輸出資料集，形式為矩陣，每 1 column 代表 1 個訓練樣本。\n",
    "\n",
    "l0\t網路第 1 層，即網路輸入層。\n",
    "\n",
    "l1\t網路第 2 層，常稱作隱藏層。\n",
    "\n",
    "syn0\t第一層權值，突觸 0 ，連接 l0 層與 l1 層。\n",
    "\n",
    "*\t逐元素相乘，故兩等長向量相乘等同於其對等元素分別相乘，結果為同等長度的向量。\n",
    "\n",
    "–\t元素相減，故兩等長向量相減等同於其對等元素分別相減，結果為同等長度的向量。\n",
    "\n",
    "x.dot(y)\t若 x 和 y 為向量，則進行點積操作；若均為矩陣，則進行矩陣相乘操作；若其中之一為矩陣，則進行向量與矩陣相乘操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# Sigmoid 函數可以將任何值都映射到一個位於 0 到  1 範圍內的值。通過它，我們可以將實數轉化為概率值\n",
    "'''\n",
    "定義數學函數:\n",
    "y=f(x) = 1/(1+np.exp(-x));\n",
    "dy/dx = df = y*(1-y) \n",
    "\n",
    "為了計算方便, 這邊把 y=f(x) 與 dy/dx 放在同一function 裡面;\n",
    "利用 deriv (derivative)做變數, 來分別指定方程式\n",
    "\n",
    "'''    \n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "'''\n",
    "通過 “nonlin” 函數體還能得到 sigmod 函數的導數（當形參 deriv 為 True 時）。\n",
    "Sigmoid 函數優異特性之一，在於只用它的輸出值便可以得到其導數值。\n",
    "若 Sigmoid 的輸出值用變數 out 表示，則其導數值可簡單通過式子 out *(1-out) \n",
    "'''\n",
    "\n",
    "'''\n",
    "input dataset\n",
    "輸入資料集初始化為 numpy 中的矩陣。每一 row 為一個“訓練實例”，\n",
    "每一 column 的對應著一個輸入節點。這樣，我們的神經網路便有 3 個輸入節點，\n",
    "4 個訓練實例。\n",
    "'''\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])  \n",
    "        \n",
    "# define output dataset \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "'''\n",
    "輸出資料集進行初始化\n",
    ".T” 為轉置函數。經轉置後，該  y  矩陣便包含 4 row 1 column。\n",
    "同我們的輸入一致，每一 row 是一個訓練實例，而每一column（僅有一column）對應一個輸出節點。\n",
    "因此，我們的網路含有 3 個輸入， 1 個輸出\n",
    "'''\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "#亂數設定產生種子得到的權重初始化集仍是隨機分佈的，\n",
    "#但每次開始訓練時，得到的權重初始集分佈都是完全一致的。\n",
    " \n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "iter = 0\n",
    "\n",
    "syn0_history = [syn0]\n",
    "#該神經網路權重矩陣的初始化操作。\n",
    "#用 “syn0” 來代指 (即“輸入層-第一層隱層”間權重矩陣）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神經網路訓練\n",
    "for 迴圈反覆運算式地多次執行訓練代碼，使得我們的網路能更好地擬合訓練集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "\n",
      "\n",
      "[[0.00966449]\n",
      " [0.00786506]\n",
      " [0.99358898]\n",
      " [0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    \n",
    "    '''\n",
    "    首先讓網路基於給定輸入“試著”去預測輸出。\n",
    "    以至於作出一些調整，使得在每次反覆運算過程中網路能夠表現地更好一點。\n",
    "    (4 x 3) dot (3 x 1) = (4 x 1)\n",
    "    此 column 代碼包含兩個步驟。首先，將 l0 與 syn0 進行矩陣相乘。\n",
    "    然後，將計算結果傳遞給 sigmoid 函數。具體考慮到各個矩陣的維度：\n",
    "    (4 x 3) dot (3 x 1) = (4 x 1)\n",
    "    '''\n",
    " \n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    " \n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "    \n",
    "    '''\n",
    "    對於每一輸入，可知 l1 都有對應的一個“猜測”結果。那麼通過將真實的結果（y）與猜測結果（l1）作減，\n",
    "    就可以對比得到網路預測的效果怎麼樣。\n",
    "    l1_error 是一個有正數和負數組成的向量，它可以反映出網路的誤差有多大\n",
    "    '''\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "    \n",
    "print(\"Output After Training:\")\n",
    "print(\"\\n\")\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Backpropagation](https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業重點: \n",
    "\n",
    "3  層神經網路\n",
    "\n",
    "通過增加更多的中間層，以對更多關係的組合進行建模\n",
    "\n",
    "syn1 權值矩陣將隱層的組合輸出映射到最終結果，\n",
    "\n",
    "而在更新 syn1 的同時，還需要更新 syn0 權值矩陣，\n",
    "\n",
    "以從輸入資料中更好地產生這些組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# Sigmoid 函數可以將任何值都映射到一個位於 0 到  1 範圍內的值。通過它，我們可以將實數轉化為概率值\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])  \n",
    "        \n",
    "# define y for output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神經網路訓練\n",
    "for 迴圈反覆運算式地多次執行訓練代碼，使得我們的網路能更好地擬合訓練集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "#亂數設定產生種子得到的權重初始化集仍是隨機分佈的，\n",
    "#但每次開始訓練時，得到的權重初始集分佈都是完全一致的。\n",
    " \n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "# define syn1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "iter = 0\n",
    "#該神經網路權重矩陣的初始化操作。\n",
    "#用 “syn0” 來代指 (即“輸入層-第一層隱層”間權重矩陣）\n",
    "#用 “syn1” 來代指 (即“輸入層-第二層隱層”間權重矩陣）\n",
    "\n",
    "神經網路訓練\n",
    "for 迴圈反覆運算式地多次執行訓練代碼，使得我們的網路能更好地擬合訓練集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[0.00510229]\n",
      " [0.00421887]\n",
      " [0.99493875]\n",
      " [0.99437164]]\n"
     ]
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    # forward propagation\n",
    "    L0 = X\n",
    "    L1 = nonlin(np.dot(L0,syn0))\n",
    "    # (4 x 3) dot (3 x 3) = (4 x 3)\n",
    "    \n",
    "    L2 = nonlin(np.dot(L1,syn1))\n",
    "    # (4 x 3) dot (3 x 1) = (4 x 1)\n",
    "    \n",
    "    '''\n",
    "     l2_error 該值說明了神經網路預測時“丟失”的數目。\n",
    "     l2_delta 該值為經確信度加權後的神經網路的誤差，除了確信誤差很小時，它近似等於預測誤差。\n",
    "     l1_error 該值為 l2_delta 經 syn1 加權後的結果，從而能夠計算得到中間層/隱層的誤差。\n",
    "     l1_delta 該值為經確信度加權後的神經網路 l1 層的誤差，除了確信誤差很小時，它近似等於 l1_error 。\n",
    "    '''\n",
    " \n",
    "    # how much did we miss?\n",
    "    L2_error = y - L2\n",
    "    L2_delta = L2_error * nonlin(L2,True)\n",
    "    \n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    L1_error = L2_delta.dot(syn1.T)\n",
    " \n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in L1\n",
    "    \n",
    "    L1_delta = L1_error * nonlin(L1,True)\n",
    "    \n",
    "    # update weights\n",
    "    syn0 += np.dot(L0.T,L1_delta)\n",
    "     # syn1 update weights\n",
    "    syn1 += np.dot(L1.T,L2_delta)\n",
    "print(\"Output After Training:\")\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
